<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | IMM Laboratory</title>

<link rel="icon" href="/preview/pr-9/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="IMM Laboratory">
<meta property="og:description" content="">
<meta property="og:url" content="">
<meta property="og:image" content="/preview/pr-9/images/share.png">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/preview/pr-9/images/share.png">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/preview/pr-9/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/preview/pr-9/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/preview/pr-9/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/all.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/background.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/body.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/button.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/card.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/code.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/float.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/font.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/form.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/header.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/image.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/link.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/list.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/main.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/section.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/table.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/preview/pr-9/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/preview/pr-9/_scripts/anchors.js"></script>

  <script src="/preview/pr-9/_scripts/dark-mode.js"></script>

  <script src="/preview/pr-9/_scripts/fetch-tags.js"></script>

  <script src="/preview/pr-9/_scripts/search.js"></script>

  <script src="/preview/pr-9/_scripts/site-search.js"></script>

  <script src="/preview/pr-9/_scripts/tooltip.js"></script>


</head>
  <body>
    





<header class="background" style="--image: url('/preview/pr-9/images/background.jpg')">
  <a href="/preview/pr-9/" class="home">
    
      <span class="logo">
        
          <img src="/preview/pr-9/images/logo.png" alt="logo">
        
      </span>
    
    
      <span class="title" data-tooltip="Home">
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/preview/pr-9/Research/" data-tooltip="Research aims and studies">
          Research
        </a>
      
    
      
        <a href="/preview/pr-9/Publications/" data-tooltip="Research papers and PDFs">
          Publications
        </a>
      
    
      
        <a href="/preview/pr-9/People/" data-tooltip="About our team">
          People
        </a>
      
    
      
        <a href="/preview/pr-9/Opportunities/" data-tooltip="Research participants and open positions">
          Opportunities
        </a>
      
    
      
        <a href="/preview/pr-9/Gallery/" data-tooltip="Lab photos and events">
          Gallery
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 id="research">
<i class="icon fa-solid fa-microscope"></i>Research</h1>

<p><br> 
<span style="font-size:1.25em;"> 1. <a href="#1-ensemble-coding-for-fast-vision%E2%80%93action-interactions">Ensemble coding for fast vision–action interactions</a> <br> <br> 2. <a href="#2-joint-visuomotor-movements-in-real-world-settings-working-with-human-or-robot-co-workers-for-common-action-goals-in-construction-sites">Joint visuomotor movements in real world settings: Working with human or robot co-workers for common goals</a> <br><br> 3. <a href="#3-visuomotor-abilities-in-children-development-of-vision-and-action-links-and-impacts-of-early-neurodevelopment-disordersx">Visuomotor abilities in children</a> <br><br> 4. <a href="#4-multimodal-neuroimaging-for-a-better-understanding-of-temporal-and-spatial-properties-of-brain-dynamics-during-visually-guided-movements">Multimodal neuroimaging for a better understanding of temporal and spatial properties of brain dynamics</a></span></p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="1-ensemble-coding-for-fast-visionaction-interactions"><strong>(1) Ensemble coding for fast vision–action interactions</strong></h2>

<p style="font-size:17px">
You never see the same world twice: Visual inputs from the world constantly change, and our actions are continually in motion (at least our eyes) even when trying to remain still. Everytime we open our eyes,  our brain receives a barrage of new information from the complex and cluttered visual environment. Our brain manages this surprisingly well; It condenses and reconstructs images by grouping parts (e.g., objects) into meaningful sets (e.g., ensembles), relying on similarity, redundancy, or structural regularity in the images. <i> Ensemble coding </i> operates to dilute high-level descriptions into "summary representations."
</p>
<p><br></p>

<p style="font-size:17px">
Ensemble coding is an excellent example of adaptive visual processes the brain uses to lighten its cognitive workload required for processing and remembering all details of images. Studies have shown that people are incredibly adept at perceiving ensembles made of various features, including size, motion, orientation, and facial emotions or identities. Particularly, ensemble coding's utility is in its processing speed, instantly creating coherent snapshots of the ever-changing visual world. It can then initiate instant action commands based on snap judgments about the current environment: <i> e.g., Should I move away from those grumpy people surrounding me? Which plate to grab to get more cookies before someone else takes it? </i> 
</p>
<p><br></p>

<p style="font-size:17px">
We are currently studying how different parts of the brain work during this dynamic processes where visual ensembles are perceived and guide our movements for interacting with and navigating the world. 
</p>
<p><br></p>

<p><em>*This work is funded by the Natural Sciences and Engineering Council of Canada and the National Research Foundation of Korea.</em>
<br></p>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/preview/pr-9/images/Fig1-Research-new2.png" style="
        width: auto;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/preview/pr-9/images/fallback.svg'; this.onerror = null;">
  </a>
  
    <figcaption class="figure-caption">
      

    </figcaption>
  
</figure>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="2-joint-visuomotor-movements-in-real-world-settings-working-with-human-or-robot-co-workers-for-common-action-goals-in-construction-sites"><strong>(2) Joint visuomotor movements in real-world settings: Working with human or robot co-workers for common action goals in construction sites</strong></h2>

<p style="font-size:17px">
We often make goal-directed movements, not by ourselves, but in collaboration with others, such as lifting and moving a heavy object or hanging a big picture frame on the wall. We study how the human brain works to mediate interactive and collaborative visuomotor movements. For joint movements, not all actions are performed by one person; a series of complex movements is often partitioned into discrete and simpler segments, then different portions are assigned to each person. From one person's perspective, their individual actions wouldn't make up a meaningful set of continuous movements: Instead, there should be discontinuity, pauses, or rushes to temporally coordinate different actions and synchronize work dynamics with others. For joint movements, therefore, they must be able to understand others' minds, action intentions, goals, and current progress, relying on cognitive, behavioural, emotional, and linguistic cues. 
</p>
<p><br></p>

<p style="font-size:17px">
Performing joint movements is integral to dynamic workflows in many industrial settings, such as construction and maintenance sites. Successfully planning and executing joint movements in the correct order and at the right time is vital for workers' safety and product quality. As a part of a collaborative group with researchers from Civil Engineering, Mechanical Engineering, Biomedical Engineering, and Computer Science, we are currently exploring cognitive, behavioural, and neural processes during successful and unsuccessful collaborative visuomotor movements in the real-world scenarios of construction sites, created by virtual reality and mobile labs equipped with wearable sensing devices. As a neuroscience research team, we lead studies on human abilities and strategies to assess action intentions, motor capability, and emotional and cognitive states of other co-workers and then, most importantly, to adjust and modify their own action plans and execution accordingly in real-time. We use visuomotor tasks made of common and essential functional modules for achieving manipulation goals in construction sites, including reaching, grasping, lifting, rotating, pulling, etc. We examine the brain signals, behavioural performance and 3D movements, and biophysiological measures in human construction workers during these collaborative visuomotor tasks.    
</p>
<p><br></p>

<p style="font-size:17px">
Now that the construction industry is becoming more futuristic (!!), with WALL-E-like robotic workers assisting human workers in many dexterous, physically demanding, and dangerous tasks, we also study human-robot collaborations during the same visuomotor movements in virtual construction settings that are distracting, cluttered, and busy. We generate multimodality data from human construction workers, measuring cognitive/emotional states (e.g., attentional focus or distraction, anxiety, confidence level, etc.) and behaviours (e.g., common patterns observed immediately before human lapses and errors which can sometimes be fatal, such as falling or dropping). These data are processed so that they serve as useful inputs for controlling policies of human-centred construction robots being developed by our research collaborators in engineering departments. We hope this interdisciplinary research project will improve robot construction workers' abilities to read and respond to how human co-workers learn, move, feel, and make decisions on site and enhance training programs and task structures for human co-workers, making working dynamics between robot-human pairs safer and less demanding.
</p>
<p><br></p>

<p><em>*This work is funded by the Social Sciences and Humanities Research Council and the Natural Sciences and Engineering Council of Canada.</em>
<br></p>

<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/preview/pr-9/images/Fig2-Research.png" style="
        width: auto;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/preview/pr-9/images/fallback.svg'; this.onerror = null;">
  </a>
  
    <figcaption class="figure-caption">
      

    </figcaption>
  
</figure>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="3-visuomotor-abilities-in-children-development-of-vision-and-action-links-and-impacts-of-early-neurodevelopmental-disorders"><strong>(3) Visuomotor abilities in children: development of vision and action links and impacts of early neurodevelopmental disorders</strong></h2>

<p style="font-size:17px">
Visuomotor experiences during childhood play a significant role in wiring the brain and shaping lifetime interactions with the world. Learning to coordinate our eyes and body's (especially our hands and feet) by adjusting movements in response to visual feedback is an important developmental milestone for children to acquire gross and fine motor skills, such as writing, tying shoe laces, walking, and artistic or athletic activities. We study the developmental patterns of such vision and action links in children of a wide age range, from 5 to 18 years old, to better understand how early visuomotor experiences impact the way their developing brains wire and create neural connections to build "functional networks.”
</p>
<p><br></p>

<p style="font-size:17px">
Using neuroimaging devices that are non-invasive and suitable for testing children (mainly magnetoencephalography [MEG], but we use some other devices too!), we measure detailed time series of brain activities and the brain's rhythmic patterns. These measurements provide insight into how vision and action links unfold over time across different brain regions in young children with typical development or with neurodevelopment disorders. Our animated, aquarium-themed tasks are designed to be entertaining and easy enough for children to perform. The task allows us to record a rich set of continuous hand movement data over time while the children's brain activities are recorded with MEG. Combining the hand movement data and the brain time-course recordings, we are currently exploring how and when the visual and motor systems communicate during successful versus unsuccessful movements and how excitatory and inhibitory connections among different brain regions coordinate goal-directed actions.
</p>
<p><br></p>

<p style="font-size:17px">
Because children's brains are still plastic, disruption in one function (e.g., vision) will likely have far-reaching and lasting consequences on the ways that the brain's functional specialization is shaped and established in the long run. This motivates our recent projects on visuomotor skill learning in children who have reduced vision due to a neurodevelopment disorder called amblyopia (also called lazy eye) early in life. We study how reduced vision during early childhood is associated with a cascade of impacts on visuomotor abilities and subsequent changes in a range of brain functions, which must be acquired and matured during this critical period for brain and behaviour development. 
</p>
<p><br></p>

<p><em>*This work is funded by the BC Children’s Hospital Research Institute, Brain, Behaviour, and Development Theme and UBC Health.</em>
<br></p>
<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/preview/pr-9/images/Fig3-Research.png" style="
        width: auto;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/preview/pr-9/images/fallback.svg'; this.onerror = null;">
  </a>
  
    <figcaption class="figure-caption">
      

    </figcaption>
  
</figure>

<p><br></p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<h2 id="4-multimodal-neuroimaging-for-a-better-understanding-of-temporal-and-spatial-properties-of-brain-dynamics-during-visually-guided-movements"><strong>(4) Multimodal neuroimaging for a better understanding of temporal and spatial properties of brain dynamics during visually-guided movements</strong></h2>

<p style="font-size:17px">
Brain recording and functional neuroimaging techniques (e.g., magnetoencephalography [MEG] and functional MRI [fMRI]) are powerful and noninvasive tools to examine typical and atypical brain function and development, informing researchers of neural mechanisms underlying behaviours and possible neural anomalies due to disorders. However, none of these methods directly measure neuroelectrical or neurochemical processes mediating brain function. Brain "activity" must be inferred from other metrics such as magnetic fields (e.g., MEG) or hemodynamics (e.g., fMRI). To overcome this limitation, multimodal approaches combining neuroimaging devices such as fMRI and MEG can be useful, providing complementary information. fMRI can characterize brain activity with sub-millimetre spatial resolution, but has limited temporal resolution; MEG can characterize neural activity on the millisecond timescale at which the brain operates but with a limited spatial resolution. Combining MEG and fMRI has great potential for providing insights into the dynamic brain function underlying behaviours and neurocognitive biomarkers of pathological brain changes. 

<br>
<br>

For the fusion of MEG and fMRI to be useful in obtaining high spatial and temporal resolution, the two signals should reflect the same underlying neural events. Previous work has suggested that neuromagnetic (MEG) and hemodynamic (fMRI) signals originate from post-synaptic currents and tend to correlate with each other. Our previous work also provided evidence for similar, overlapping patterns of whole-brain MEG and fMRI activations in healthy adults during face perception. We are currently exploring robust approaches to combining time-course MEG and fMRI data from the same participants who perform the same tasks in the MEG and MRI scanners in separate sessions. We hope that this work will provide a comprehensive understanding of when and where neural activity occurs during cognitive and motor tasks. 

<br>
<br>
For mobile studies using our recent virtual reality setup, we alternatively use electroencephalography (EEG; better temporal resolution) combined with functional near-infrared spectroscopy (fNIRS; better spatial resolution), instead of huge MEG (better temporal resolution) and fMRI (better spatial resolution) scanners, so that participants can make movements with more freedom in less restricted settings. 
</p>
<p><br></p>

<p><em>*This work is funded by the Djavad Mowafaghian Centre for Brain Health and the Social Sciences and Humanities Research Council.</em></p>
<figure class="figure">
  <a class="figure-image" aria-label="figure link">
    <img src="/preview/pr-9/images/Fig4-Research.png" style="
        width: auto;
        max-height: unset;
      " alt="figure image" loading="lazy" onerror="this.src = '/preview/pr-9/images/fallback.svg'; this.onerror = null;">
  </a>
  
    <figcaption class="figure-caption">
      

    </figcaption>
  
</figure>
  </section>


    </main>
    


<footer class="background" style="--image: url('/preview/pr-9/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-9379-1347" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=Zq3Z-ioAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.bcchr.ca/research" data-tooltip="BC Children's Hospital webpage" data-style="bare" aria-label="BC Children's Hospital webpage">
      <i class="icon fa-solid fa-hospital"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://psych.ubc.ca/" data-tooltip="University of British Columbia Psychology" data-style="bare" aria-label="University of British Columbia Psychology">
      <i class="icon fa-solid fa-database"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://vision.ubc.ca/" data-tooltip="UBC Vision" data-style="bare" aria-label="UBC Vision">
      <i class="icon fa-regular fa-eye"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://languagesciences.ubc.ca/" data-tooltip="UBC Language Sciences Institute" data-style="bare" aria-label="UBC Language Sciences Institute">
      <i class="icon fa-solid fa-globe"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://www.centreforbrainhealth.ca/" data-tooltip="Centre for Brain Health" data-style="bare" aria-label="Centre for Brain Health">
      <i class="icon fa-solid fa-microscope"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2024
    IMM Laboratory
      |   Template:
    <a href="https://github.com/greenelab/lab-website-template">
      Greene Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
