<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-05-21T17:59:47+00:00</updated><id>/feed.xml</id><title type="html">IMM Laboratory</title><entry><title type="html">The effect of masks on the emotion perception of a facial crowd</title><link href="/2023/08/31/publication-20.html" rel="alternate" type="text/html" title="The effect of masks on the emotion perception of a facial crowd" /><published>2023-08-31T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2023/08/31/publication-20</id><content type="html" xml:base="/2023/08/31/publication-20.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>The present study investigated the effect of facial masks on people’s ability to perceive emotions in crowds. We presented faces with the bottom halves occluded by masks or full faces without occlusion. In two sequentially presented crowds, we varied the number of faces, emotional valence, and intensity of facial expressions, examining the impact of masks on the perception of crowd emotion. Participants reported which of the two crowds they would avoid based on the crowds’ average emotions. The participants’ ability to judge the average emotion of a crowd, especially a crowd expressing happiness, was impaired when the crowd wore masks. For faces covered by masks, crowd emotion judgments were more negatively biased than those without masks. However, participants could still distinguish the emotional intensities of a crowd wearing masks above chance. Additionally, participants responded more quickly to a crowd with more people without compromising accuracy, despite the perceptual challenges imposed by facial masks. Our results suggest that under ambiguous social situations in which individuals’ emotions are partially hidden by masks, a large group may provide stronger social cues than a small group, thereby promoting communication and regulating social behaviors.</p>

<p><br /></p>

<p><br /></p>

<p><em>Cho, J., Im, H.Y., Yoon, Y.J. et al. The effect of masks on the emotion perception of a facial crowd. Sci Rep 13, 14274 (2023). https://doi.org/10.1038/s41598-023-41366-0</em>   <a href="https://doi.org/10.1038/s41598-023-41366-0" target="_blank">[PDF]</a></p>

<!---
2022
https://journals.physiology.org/doi/abs/10.1152/jn.00463.2021
-->]]></content><author><name>Cho, J., Im, H.Y., Yoon, Y.J., Joo, S.J., and Chong, S.C.</name></author><category term="ensemble perception" /><category term="psychophysics" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Work Learn Program UBC Vancouver</title><link href="/2023/06/17/WL-RA-posting1.html" rel="alternate" type="text/html" title="Work Learn Program UBC Vancouver" /><published>2023-06-17T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2023/06/17/WL-RA-posting1</id><content type="html" xml:base="/2023/06/17/WL-RA-posting1.html"><![CDATA[<p>This is a work-learn research assistant position at the IMM Lab.</p>

<h3 id="about-the-position">About the position</h3>
<p>Posted: <em>2023, July 05</em></p>

<p>The Research Assistant (RA) will work closely with the principal Investigator on human neuroimaging studies that employ magnetoencephalography (MEG) and magnetic resonance imaging (MRI) to explore dynamic changes happening in the human brain while learning visually-guided motor skills or viewing complex visual images. This position will provide a unique opportunity to assist in a research program that utilizes cutting-edge neuroimaging techniques to understand human brain functions and behaviour. The research studies will take place in different locations: the BC Children’s Hospital, UBC Psychology, and/or the Surrey Memorial Hospital (Simon Fraser University).</p>

<h3 id="duties-and-responsibilities">Duties and Responsibilities</h3>

<p>The tasks RA will be working on will include:</p>

<p>       (1) Preparing and setting up experiments</p>

<p>       (2) Contacting potential participants and obtaining consent from volunteering participants in the lab</p>

<p>       (3) Running participants (mainly school-aged children, but sometimes including adults) through various cognitive tasks designed like computer games</p>

<p>       (4) Assisting with MRI and MEG experiments.</p>

<p>       (5) Preprocessing the neuroimaging data and analyzing behavioural data</p>

<p>       (6) Literature review on topics related to research projects</p>

<h3 id="qualifications">Qualifications</h3>

<p><strong>Skills/Knowledge:</strong> Experience with the Mac/Linux operating system, Excellent oral communication and interpersonal skills. Applicants with Matlab, Python, or R programming experiences will
be preferred (although not required).</p>

<p><strong>Education level:</strong> Currently enrolled in undergraduate or graduate studies. Psychology, Neuroscience, and Engineering majors are preferred.</p>

<p><strong>Great fit:</strong> Experience with school-aged children is preferred. Ability to work effectively independently and in a team environment</p>

<p><br />
Must be able to work up to 10 hours/week during weekdays (between 9 am and 5 pm). Ability to work in two different research sites (BC Children’s Hospital in Vancouver and Surrey Memorial Hospitalin Surrey)</p>

<p><strong>Experience Level:</strong> Current Students in a UBC Undergraduate Program, Current Students in a Master’s Program</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This is a work-learn research assistant position at the IMM Lab.]]></summary></entry><entry><title type="html">[2] Neuroimaging research on human visual perception and action – Postdoctoral Fellow</title><link href="/2023/06/16/PD-posting1.html" rel="alternate" type="text/html" title="[2] Neuroimaging research on human visual perception and action – Postdoctoral Fellow" /><published>2023-06-16T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2023/06/16/PD-posting1</id><content type="html" xml:base="/2023/06/16/PD-posting1.html"><![CDATA[<h3 id="about-the-position">About the position</h3>
<p>Posted: <em>2023, July</em></p>

<p><br />
We invite applications for a Postdoctoral Fellow (PDF) with particular interests in multimodal neuroimaging studies on human visuomotor behaviours. The successful candidate will work on interdisciplinary projects led by Dr. Hee-Yeon Im (UBC Psychology), focusing on:</p>

<ul>
  <li>Cognitive and motor processes underlying goal-directed movements in distracting and collaborative work environments (e.g., construction sites)</li>
  <li>The human ability to learn new cognitive and motor skills to coordinate with robot collaborators</li>
  <li>Behavioural and neural predictors of human errors (e.g., lapses, falls, misses, etc.)</li>
</ul>

<p>Due to the interdisciplinary nature of the projects, the successful candidate will closely work with research teams from multiple departments, including Civil (Drs. Zhengbo Zou, Omar Swei, and
Qian Chen), Biomedical (Dr. Calvin Kuo), and Mechanical Engineering (Dr. Kefei Wen and Lyndia Wu) as well as Computer Science (Dr. Helge Rhodin). This position is for one year, renewable for a second year. Preferred start dates are September-October, 2023.
The salary is $52,000 per year plus benefits.</p>

<h3 id="responsbilities-and-opportunities">Responsbilities and Opportunities</h3>

<p>Responsibilities include:</p>
<ol>
  <li>contributing to new study designs and implementation;</li>
  <li>collecting behavioural, neural, and biophysiological data;</li>
  <li>leading
teamwork;</li>
  <li>mentoring students; and</li>
  <li>engaging in the dissemination of findings.</li>
</ol>

<p>The candidate will be supported to actively work on their publication record and present findings at international conferences. They will also have various opportunities for professional development
and networking with academic and industrial collaborators. Our cross-department research team is also deeply committed to providing trainees in different career stages with robust and tailored professional development programs, partnering with professional
consultants (e.g., Hikma Strategies). We have access to excellent resources for conducting neuroscience research with real-world applications, including MEG, fMRI, EEG combined with fNIRS, eye-tracker, as well as virtual/augmented reality, biosensing and motion
capture set-ups.</p>

<h3 id="qualifications">Qualifications</h3>

<ul>
  <li>A Ph.D. in Psychology, Neuroscience, Cognitive Science, Biomedical Engineering, Kinesiology, or Health sciences and strong quantitative and analytic skills.</li>
  <li>Proficiency in programming languages (e.g., Matlab and Python).</li>
  <li>Ability to work collaboratively on multidisciplinary projects and independently with excellent problem-solving and people skills.-</li>
  <li>Strong writing skills, including experience publishing academic journal articles and conference proceedings.</li>
  <li>Experience in human neurophysiological recording (e.g., MEG/EEG) and/or brain imaging (e.g., fMRI) is desired.</li>
</ul>

<p>Equity and diversity are essential to academic excellence. An open and diverse community fosters the inclusion of voices that have been underrepresented or discouraged. We encourage applications from members of groups that have been marginalized on any grounds enumerated under the B.C. Human Rights Code, including sex, sexual orientation, gender identity or expression, racialization, disability, political belief, religion, marital or family status, age, and/or status as a First Nation, Metis, Inuit, or Indigenous person.</p>

<p>UBC hires based on merit and is committed to employment equity. We encourage all qualified applicants to apply; however, Canadian citizens and permanent residents of Canada will be given priority.</p>

<p><br />
To apply, please submit a cover letter, curriculum vitae, contact details of two referees, and date of availability to Hee Yeon Im (heeyeon.im@ubc.ca). Review of applications will continue until the position is filled.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[About the position Posted: 2023, July]]></summary></entry><entry><title type="html">[1] Multimodal neuroimaging research on human visuomotor behaviours in real-world settings – Postdoctoral Fellow</title><link href="/2023/03/15/PD-posting1.html" rel="alternate" type="text/html" title="[1] Multimodal neuroimaging research on human visuomotor behaviours in real-world settings – Postdoctoral Fellow" /><published>2023-03-15T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2023/03/15/PD-posting1</id><content type="html" xml:base="/2023/03/15/PD-posting1.html"><![CDATA[<p>This post-doctoral position is for neuroimaging research on human visuomotor behaviours in real-world environments at the IMM Lab.</p>

<h3 id="about-the-position">About the position</h3>
<p><em>2023, June 16</em></p>

<p>We invite applications for a Postdoctoral Fellow (PDF) with particular interests in neuroimaging studies (fMRI and MEG) on visual perception and action in humans. The successful candidate will
work on research projects led by Dr. Hee-Yeon Im (UBC Psychology) and Dr. Deborah Giaschi (UBC Ophthalmology and Visual Sciences), focusing on:</p>

<ul>
  <li>Neural correlates of visual perception (e.g., motion, depth, faces) and visually-guided action in adults and in children with typical or neurodivergent development (e.g., amblyopia or dyslexia)</li>
  <li>Integrating task-based and resting-state fMRI for transdiagnostic investigation in pediatric populations</li>
  <li>Developing and using new approaches to combining task-based fMRI and MEG.</li>
</ul>

<p>This position is for one year, with the possibility of renewal for a second year. Preferred start date is September-October, 2023. The salary is $52,000 per year plus benefits.</p>

<h3 id="responsibilities-and-opportunities">Responsibilities and Opportunities:</h3>

<p>Responsibilities include:</p>
<ol>
  <li>contributing to new study design and implementation;</li>
  <li>collecting and analyzing behavioural and neural data;</li>
  <li>leading teamwork;</li>
  <li>mentoring students; and</li>
  <li>engaging in the dissemination of findings.</li>
</ol>

<p>The candidate will be supported to actively work on their publication record and present findings at international conferences. They will also have various opportunities for professional development
and networking with academic collaborators. They will have access to excellent resources for conducting neuroscience research, including MEG, fMRI, and eye and hand motion trackers. The candidate will also have opportunities to expand their learning and networking
through the UBC Research Excellence Cluster in Vision and BC Children’s Hospital Research Institute.</p>

<h3 id="qualifications-and-experience">Qualifications and Experience</h3>

<ul>
  <li>A Ph.D. in Psychology, Neuroscience, Vision Science, Cognitive Science or a related field</li>
  <li>Proficiency in programming languages (e.g., Matlab and Python) and strong quantitative and analytic skills.</li>
  <li>Ability to work collaboratively on multidisciplinary projects and independently with excellent problem-solving and people skills.</li>
  <li>Strong writing skills, including experience publishing academic journal articles and conference proceedings.</li>
  <li>Experience in functional MRI with human participants (preferably children).</li>
</ul>

<p><br /></p>

<p>Equity and diversity are essential to academic excellence. An open and diverse community fosters the inclusion of voices that have been underrepresented or discouraged. We encourage applications from members of groups that have been marginalized on any grounds enumerated under the B.C. Human Rights Code, including sex, sexual orientation, gender identity or expression, racialization, disability, political belief, religion, marital or family status, age, and/or status as a First Nation, Metis, Inuit, or Indigenous person.</p>

<p>UBC hires based on merit and is committed to employment equity. We encourage all qualified applicants to apply; however, Canadian citizens and permanent residents of Canada will be given priority.</p>

<p><br />
<br /></p>

<p>To apply, please submit a cover letter, curriculum vitae, contact details of two referees, and date of availability to Hee Yeon Im (heeyeon.im@ubc.ca). Review of applications will continue until the position is filled.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post-doctoral position is for neuroimaging research on human visuomotor behaviours in real-world environments at the IMM Lab.]]></summary></entry><entry><title type="html">Inconsistent attentional contexts impair relearning following gradual visuomotor adaptation</title><link href="/2022/09/01/publication-19.html" rel="alternate" type="text/html" title="Inconsistent attentional contexts impair relearning following gradual visuomotor adaptation" /><published>2022-09-01T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2022/09/01/publication-19</id><content type="html" xml:base="/2022/09/01/publication-19.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>One of the brain’s primary functions is to promote actions in dynamic, distracting environments. Because distractions divert attention from our primary goals, we must learn to maintain accurate actions under sensory and cognitive distractions. Visuomotor adaptation is a learning process that restores performance when sensorimotor capacities or environmental conditions are abruptly or gradually altered. Prior work showed that learning to counteract an abrupt perturbation under a particular single- or dual-task setting (i.e., attentional context) was associated with better recall under the same conditions. This suggested that the attentional context was encoded during adaptation and used as a recall cue. The current study investigated whether the attentional context (i.e., single vs. dual task) also affected adaptation and recall to a gradual perturbation, which limited awareness of movement errors. During adaptation, participants moved a cursor to a target while learning to counteract a visuomotor rotation that increased from 0 to 45 by 0.3 each trial, with or without performing a secondary task. Relearning was impaired when the attentional context was different between adaptation and recall (experiment 1), even when the exposure to the attentional context was limited to the early or late half of adaptation (experiment 2). Changing the secondary task did not affect relearning, indicating that the attentional context, rather than specific stimuli or tasks, was associated with better recall performance (experiment 3). These findings highlight the importance of cognitive factors, such as attention, in visuomotor adaptation and have implications for learning and rehabilitation paradigms.</p>

<p><br /></p>

<p><strong><em>NEW &amp; NOTEWORTHY</em></strong> Adaptation acquired under single- or dual-task setting, which created an undivided or divided attentional
context, respectively, was impaired when relearning occurred under different conditions (i.e., shifting from a dual to single task).
Changes to the attentional context impaired relearning when the initial adaptation was to a gradual perturbation. Explicit awareness of the perturbation was not necessary for this effect to be robust, nor was the effect attributable to changes in the secondary task requirements.</p>

<p><br /></p>

<p>*Im, H. Y., Liddy, J. J., &amp; Song, J.-H. (2022). Inconsistent attentional contexts impair relearning following gradual visuomotor adaptation. Journal of Neurophysiology, 128(3), 527–542. https://doi.org/10.1152/jn.00463.2021 *<a href="https://doi.org/10.1152/jn.00463.2021" target="_blank">[PDF]</a></p>

<!---
2022
https://journals.physiology.org/doi/abs/10.1152/jn.00463.2021
-->]]></content><author><name>Im, H. Y., Liddy, J. J., &amp; Song, J.-H.</name></author><category term="visuomotor learning" /><category term="hand movement tracking" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds</title><link href="/2021/02/03/publication-18.html" rel="alternate" type="text/html" title="Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds" /><published>2021-02-03T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2021/02/03/publication-18</id><content type="html" xml:base="/2021/02/03/publication-18.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>Reading the prevailing emotion of groups of people (“crowd emotion”) is critical to understanding their overall intention and disposition. It alerts us to potential dangers, such as angry mobs or panicked crowds, giving us time to escape. A critical aspect of processing crowd emotion is that it must occur rapidly, because delays often are costly. Although knowing the timing of neural events is crucial for understanding how the brain guides behaviors using coherent signals from a glimpse of multiple faces, this information is currently lacking in the literature on face ensemble coding. Therefore, we used magnetoencephalography to examine the neurodynamics in the dorsal and ventral visual streams and the periamygdaloid cortex to compare perception of groups of faces versus individual faces. Forty-six participants compared two groups of four faces or two individual faces with varying emotional expressions and chose which group or individual they would avoid. We found that the dorsal stream was activated as early as 68 msec after the onset of stimuli containing groups of faces. In contrast, the ventral stream was activated later and predominantly for individual face stimuli. The latencies of the dorsal stream activation peaks correlated with participants’ response times for facial crowds. We also found enhanced connectivity earlier between the periamygdaloid cortex and the dorsal stream regions for crowd emotion perception. Our findings suggest that ensemble coding of facial crowds proceeds rapidly and in parallel by engaging the dorsal stream to mediate adaptive social behaviors, via a distinct route from single face perception.
<br />
<br /></p>

<p><em>Im, H.Y., Cushing, C., Ward, N., &amp; Kveraga, K. (2021). Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds and individuals: a MEG study. Cognitive, Affective, and Behavioral Neuroscience.</em> <a href="https://doi.org/10.3758/s13415-021-00880-2" target="_blank">[PDF]</a></p>

<!---
Im, H.Y., Cushing, C., Ward, N., & Kveraga, K. (2021). Differential neurodynamics and connectivity in the dorsal and ventral visual pathways during perception of emotional crowds and individuals: a MEG study. Cognitive, Affective, and Behavioral Neuroscience.
https://link.springer.com/article/10.3758/s13415-021-00880-2
14 May 2020
-->]]></content><author><name>Im, H. Y., Cushing, C., Ward, N., &amp; Kveraga, K</name></author><category term="ensemble perception" /><category term="MEG" /><category term="social vision" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">An explicit investigation of the roles that feature distributions play in rapid visual categorization</title><link href="/2020/05/14/publication-17.html" rel="alternate" type="text/html" title="An explicit investigation of the roles that feature distributions play in rapid visual categorization" /><published>2020-05-14T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2020/05/14/publication-17</id><content type="html" xml:base="/2020/05/14/publication-17.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>Ensemble representations are often described as efficient tools when summarizing features of multiple similar objects as a group. However, it can sometimes be more useful not to compute a single summary description for all of the objects if they are substantially different, for example when they belong to entirely different categories. It was proposed that the visual system can efficiently use the distributional information of ensembles to decide whether simultaneously displayed items belong to single or several different categories. Here we directly tested how the feature distribution of items in a visual array affects an ability to discriminate individual items (Experiment 1) and sets (Experiments 2–3) when participants were instructed explicitly to categorize individual objects based on the median of size distribution. We varied the width (narrow or fat) as well as the shape (smooth or two-peaked) of distributions in order to manipulate the ease of ensemble extraction from the items. We found that observers unintentionally relied on the grand mean as a natural categorical boundary and that their categorization accuracy increased as a function of the size differences among individual items and a function of their separation from the grand mean. For ensembles drawn from two-peaked size distributions, participants showed better categorization performance. They were more accurate at judging within-category ensemble properties in other dimensions (centroid and orientation) and less biased by superset statistics. This finding corroborates the idea that the two-peaked feature distributions support the “segmentability” of spatially intermixed sets of objects. Our results emphasize important roles of ensemble statistics (mean, range, distribution shape) in explicit visual categorization.
<br /><br /></p>

<p><em>Im, H.Y., Tiurina, N.A., &amp; Utochkin, I.S. (2020). An explicit investigation of the roles that feature distributions play in rapid visual categorization. Attention, Perception, &amp; Psychophysics.</em>  <a href="https://doi.org/10.3758/s13414-020-02046-7" target="_blank">[PDF]</a>
<!---
Im, H.Y., Tiurina, N.A., & Utochkin, I.S. (2020). An explicit investigation of the roles that feature distributions play in rapid visual categorization. Attention, Perception, & Psychophysics.
https://link.springer.com/article/10.3758/s13414-020-02046-7
14 May 2020
--></p>]]></content><author><name>Im, H. Y., Tiurina, N. A., &amp; Utochkin, I. S.</name></author><category term="ensemble perception" /><category term="psychophysics" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Fast saccadic and manual responses to faces presented to the koniocellular visual pathway</title><link href="/2020/02/25/publication-16.html" rel="alternate" type="text/html" title="Fast saccadic and manual responses to faces presented to the koniocellular visual pathway" /><published>2020-02-25T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2020/02/25/publication-16</id><content type="html" xml:base="/2020/02/25/publication-16.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>The parallel pathways of the human visual system differ in their tuning to luminance, color, and spatial frequency. These attunements recently have been shown to propagate to differential processing of higher-order stimuli, facial threat cues, in the magnocellular (M) and parvocellular (P) pathways, with greater sensitivity to clear and ambiguous threat, respectively. The role of the third, koniocellular (K) pathway in facial threat processing, however, remains unknown. To address this gap in knowledge, we briefly presented peripheral face stimuli psychophysically biased towards M, P, or K pathways. Observers were instructed to report via a key-press whether the face was angry or neutral while their eye movements and manual responses were recorded. We found that short-latency saccades were made more frequently to faces presented in the K channel than to P or M channels. Saccade latencies were not significantly modulated by expressive and identity cues. In contrast, manual response latencies and accuracy were modulated by both pathway biasing and by interactions of facial expression with facial masculinity, such that angry male faces elicited the fastest, and angry female faces, the least accurate, responses. We conclude that face stimuli can evoke fast saccadic and manual responses when projected to the K pathway.
<br /><br /></p>

<p><em>Kveraga, K., Im, H.Y., Ward, N., &amp; Adams, R.B.Jr. (2020). Fast saccadic and manual responses to faces presented to the koniocellular visual pathway. Journal of Vision, 20(2):9. doi: 10.1167/jov.20.2.9.</em> <a href="https://doi.org/10.1167/jov.20.2.9" target="_blank">[PDF]</a></p>

<!---
Kveraga, K., Im, H.Y., Ward, N., & Adams, R.B.Jr. (2020). Fast saccadic and manual responses to faces presented to the koniocellular visual pathway. Journal of Vision, 20(2):9. doi: 10.1167/jov.20.2.9.
https://jov.arvojournals.org/article.aspx?articleid=2762175
25 January 2019
-->]]></content><author><name>Kveraga, K., Im, H. Y., Ward, N., &amp; Adams, R. B. Jr.</name></author><category term="social vision" /><category term="eye movement tracking" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat</title><link href="/2019/04/23/publication-15.html" rel="alternate" type="text/html" title="Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat" /><published>2019-04-23T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2019/04/23/publication-15</id><content type="html" xml:base="/2019/04/23/publication-15.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>Recently, speed of presentation of facially expressive stimuli was found to influence the processing of compound threat cues (e.g., anger/fear/gaze). For instance, greater amygdala responses were found to clear (e.g., direct gaze anger/averted gaze fear) versus ambiguous (averted gaze anger/direct gaze fear) combinations of threat cues when rapidly presented (33 and 300ms), but greater to ambiguous versus clear threat cues when presented for more sustained durations (1, 1.5, and 2 s). A working hypothesis was put forth (Adams et al., 2012) that these effects were due to differential magnocellular versus parvocellular pathways contributions to the rapid versus sustained processing of threat, respectively. To test this possibility directly here, we restricted visual stream processing in the fMRI environment using facially expressive stimuli specifically designed to bias visual input exclusively to the magnocellular versus parvocellular pathways. We found that for magnocellular-biased stimuli, activations were predominantly greater to clear versus ambiguous threat-gaze pairs (on par with that previously found for rapid presentations of threat cues), whereas activations to ambiguous versus clear threat-gaze pairs were greater for parvocellular-biased stimuli (on par with that previously found for sustained presentations). We couch these findings in an adaptive dual process account of threat perception and highlight implications for other dual process models within psychology.
<br /><br /></p>

<p><em>Adams, R.B.Jr., Im, H.Y., Cushing, C., Boshyan, J., Ward, N., Albohn, D.N., &amp; Kveraga, K. (2019). Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat. Progress in Brain Research, 247, 71-87.</em> <a href="https://doi.org/10.1016/bs.pbr.2019.03.006" target="_blank">[PDF]</a></p>

<!---
Adams, R.B.Jr., Im, H.Y., Cushing, C., Boshyan, J., Ward, N., Albohn, D.N., & Kveraga, K. (2019). Differential magnocellular versus parvocellular pathway contributions to the combinatorial processing of facial threat. Progress in Brain Research, 247, 71-87.
https://www.sciencedirect.com/science/article/pii/S0079612319300366?via%3Dihub
April 23, 2019
-->]]></content><author><name>Adams, R. B. Jr., Im, H. Y., Cushing, C., Boshyan, J., Ward, N., Albohn, D. N., &amp; Kveraga, K.</name></author><category term="social vision" /><category term="fMRI" /><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Magnocellular and parvocellular pathway contributions to facial threat cue processing</title><link href="/2019/02/13/publication-14.html" rel="alternate" type="text/html" title="Magnocellular and parvocellular pathway contributions to facial threat cue processing" /><published>2019-02-13T00:00:00+00:00</published><updated>2024-05-21T17:58:31+00:00</updated><id>/2019/02/13/publication-14</id><content type="html" xml:base="/2019/02/13/publication-14.html"><![CDATA[<p><strong>Abstract</strong></p>

<p>Human faces evolved to signal emotions, with their meaning contextualized by eye gaze. For instance, a fearful expression paired with averted gaze clearly signals both presence of threat and its probable location. Conversely, direct gaze paired with facial fear leaves the source of the fear-evoking threat ambiguous. Given that visual perception occurs in parallel streams with different processing emphases, our goal was to test a recently developed hypothesis that clear and ambiguous threat cues would differentially engage the magnocellular (M) and parvocellular (P) pathways, respectively. We employed two-tone face images to characterize the neurodynamics evoked by stimuli that were biased toward M or P pathways. Human observers (N = 57) had to identify the expression of fearful or neutral faces with direct or averted gaze while their magnetoencephalogram was recorded. Phase locking between the amygdaloid complex, orbitofrontal cortex (OFC) and fusiform gyrus increased early (0–300 ms) for M-biased clear threat cues (averted-gaze fear) in the β-band (13–30 Hz) while P-biased ambiguous threat cues (direct-gaze fear) evoked increased θ (4–8 Hz) phase locking in connections with OFC of the right hemisphere. We show that M and P pathways are relatively more sensitive toward clear and ambiguous threat processing, respectively, and characterize the neurodynamics underlying emotional face processing in the M and P pathways.
<br /><br /></p>

<p><em>Cushing, C., Im, H.Y., Adams, R.B.Jr., Ward, N., &amp; Kveraga, K. (2019). Magnocellular and parvocellular pathway contributions to facial threat cue processing. Social Cognitive and Affective Neuroscience, 14, 151-162.</em>  <a href="https://doi.org/10.1093/scan/nsz003" target="_blank">[PDF]</a></p>

<!---
Cushing, C., Im, H.Y., Adams, R.B.Jr., Ward, N., & Kveraga, K. (2019). Magnocellular and parvocellular pathway contributions to facial threat cue processing. Social Cognitive and Affective Neuroscience, 14, 151-162
https://academic.oup.com/scan/article/14/2/151/5306910
Feb 13 2019
-->]]></content><author><name>Cushing, C., Im, H. Y., Adams, R. B. Jr., Ward, N., &amp; Kveraga, K.</name></author><category term="social vision" /><category term="fMRI" /><category term="MEG" /><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>